<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Open-iSCSI | atomic-penguin's personal webpage.]]></title>
  <link href="http://atomic-penguin.github.com/blog/categories/open-iscsi/atom.xml" rel="self"/>
  <link href="http://atomic-penguin.github.com/"/>
  <updated>2012-09-09T02:20:14-04:00</updated>
  <id>http://atomic-penguin.github.com/</id>
  <author>
    <name><![CDATA[Eric G. Wolfe]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Red Hat Enterprise Linux 5.x iSCSI and Device Mapper Multipath HOWTO]]></title>
    <link href="http://atomic-penguin.github.com/blog/2010/11/29/Red-Hat-Enterprise-Linux-5.x-iSCSI-and-device-mapper-multipath-HOWTO/"/>
    <updated>2010-11-29T00:00:00-05:00</updated>
    <id>http://atomic-penguin.github.com/blog/2010/11/29/Red-Hat-Enterprise-Linux-5.x-iSCSI-and-device-mapper-multipath-HOWTO</id>
    <content type="html"><![CDATA[<h2>Abstract</h2>

<p><img src="/images/muwam.png" alt="Marshall University Systems Infrastructure Team" /></p>

<p>This document outlines in a detailed step-by-step fashion,
how to properly configure iSCSI initiator, and multi-path I/O
software on Red Hat Enterprise Linux version 5.</p>

<pre><code>Copyright © 2010, Marshall University Systems Infrastructure Team
Author: Eric G. Wolfe 
Contributor: Jaymz Mynes 
Editor: Tim Calvert 

Marshall University 
400 Hal Greer Blvd. 
Huntington, WV 25755 

This work is licensed under the Creative Commons Attribution-Noncommercial-Share 
Alike 3.0 United States License. To view a copy of this license, 
visit http://creativecommons.org/licenses/by-nc-sa/3.0/us/ 
or send a letter to Creative Commons, 171 Second Street, Suite 
300, San Francisco, California, 94105, USA. 

The block M logo is a trademark of Marshall University. All other 
trademarks are owned by their respective owners. 
</code></pre>

<!-- more -->


<h2>I. Introduction</h2>

<p>The following configuration was documented and tested on a Dell
Poweredge R710, with RedHat Enterprise Linux 5.0 x64.  The R710
server has 4 Broadcom NeteXtreme II BCM5709 Gigabit Network
Interface Cards (NIC), and is connected by Ethernet to an EqualLogic
PS6000 storage array.</p>

<p>This step-by-step guide is directly applicable to these software
and hardware components.  However, much of the software configuration
would remain the same for any Linux distribution on either X86
or AMD64 based hardware.  The most dynamic variable in any case will be
specific storage solutions.  Consider any factors that your
storage vendor may support, or recommend, before proceeding.</p>

<h2>II. Hardware preparation</h2>

<p>Appropriate hardware preparation is beyond the scope of this
document.  See vendor documentation for connecting Ethernet
cables to the storage array, and appropriate configuration
of the storage array.  According to Dell EqualLogic (2008) some
recommended guidelines are as follows:</p>

<ul>
<li>Do...

<ul>
<li>Use Cat 6 or Cat 5e TSB95 cables.</li>
<li>Use redundant switches and network paths.</li>
<li>Use Flow Control on switches and NICs</li>
<li>Use Jumbo Frames on switches and NICs</li>
<li>Use VLANs, if physically separate switches are not available.</li>
</ul>
</li>
<li>Do <strong>NOT</strong>...

<ul>
<li>Use Spanning Tree Protocol on switch ports for iSCSI connections.</li>
<li>Use Unicast storm control on a switch that handles iSCSI traffic.</li>
</ul>
</li>
</ul>


<p>See <em>PS Series Array Network Performance Guidelines</em>, or relevant
storage vendor's documentation for more information.</p>

<h2>III. Manual installation</h2>

<p>The following section outlines manual installation and configuration
of Device Mapper Multipath and Open-iSCSI initiator on Red Hat
Enterprise Linux version 5.x.  If this is your first time installing
and configuring Device Mapper Multipath, or Open-iSCSI software,
then this is the recommended method of installation and configuration.<br/>
It is possible to automate a portion of this installation and
configuration with Red Hat's Kickstart, or configuration management
engines such as Puppet.  However, that is beyond the scope of this
document.</p>

<p>The methods outlined in this guide apply directly to Red Hat Enterprise
Linux, though a major portion of the configuration items are generic
and could be applied to other Linux distributions.  The author of this
HOWTO assumes these steps will, for the most part, remain constant
for subsequent versions of Red Hat Enterprise Linux.</p>

<h3>1. Package installation and verification</h3>

<p>After performing a base install of Red Hat Enterprise v5, make
sure to upgrade the system to the latest patch-level.</p>

<h4>1. Change this line in <em>/etc/yum.conf</em> as illustrated below.</h4>

<p><code>bash
exclude=kernel*
</code></p>

<h4>2. Comment out the exclude options, so the kernel may be upgraded.</h4>

<p><code>bash
exclude=#kernel*
</code></p>

<h4>3. Run the following to upgrade to the latest patch-level.</h4>

<p><code>bash
yum -y upgrade
</code></p>

<h4>4. Run the following to install needed software.</h4>

<p><code>bash
yum -y install iscsi-initiator-utils device-mapper-multipath
</code></p>

<p>Dell Equallogic (2009) recommends using RHEL 5.4 (version 5
update 4) or newer for best performance and interoperability
with PS Series storage arrays.  Ensure the following requirements
are met before proceeding.</p>

<h4>5. Run the following to check for minimum package versions.</h4>

<p><code>bash
rpm -qa iscsi-initiator-utils device-mapper-multipath
device-mapper-multipath-&lt;version&gt;-&lt;patch-level&gt;.el5
iscsi-initiator-utils-&lt;version&gt;-&lt;patch-level&gt;.el5
</code></p>

<ul>
<li>Minimum package version for PS Series arrays

<ul>
<li>iscsi-initiator-utils-<strong>6.2.0.742-0.6</strong>.el5</li>
</ul>
</li>
<li>Minimum package version for GbE NICs

<ul>
<li>iscsi-iniator-utils-<strong>6.2.0.868-0.7</strong>.el5</li>
</ul>
</li>
<li>With SELinux enabled, you may not be able to login to iSCSI targets.
You may have to create a policy for iSCSI traffic, or disable
SELinux.</li>
</ul>


<h3>2. Selecting an I/O scheduler</h3>

<p>After updating the kernel, you will need to reboot the system,
before proceeding.  This would be a good time to pick an appropriate
I/O scheduler.  Available I/O schedulers, and the kernel option
to select them, are as follows.</p>

<ul>
<li>Completely Fair Queueing – elevator=cfq</li>
<li>Deadline – <strong>elevator=deadline</strong></li>
<li>NOOP – <strong>elevator=noop</strong></li>
<li>Anticipatory – elevator=as</li>
</ul>


<p>According to a citation in Dell Equallogic (2009), the Open-iSCSI
group reports that sometimes the <strong>NOOP</strong> scheduler works best for
iSCSI server environments.  However, if this server will be used
as an Oracle database or application server, it is standard best
practice to use the <strong>Deadline</strong> scheduler for optimal performance.</p>

<h4>Run the following command to enable the NOOP scheduler, before rebooting.</h4>

<p><code>bash
sed -e 's!/vmlinuz.*!&amp; elevator=noop!g' -i /boot/grub/menu.lst
</code></p>

<h4>If you are provisioning an Oracle server, then run the following to select Deadline.</h4>

<p><code>bash
sed -e 's!/vmlinuz.*!&amp; elevator=deadline!g' -i /boot/grub/menu.lst
</code></p>

<p>Finally, reboot the server after upgrading the system and kernel. <br/>
Do this before proceeding with the configuration steps that
follow so the proper I/O scheduler will be active and the newer
kernel will be available.  You may also want to go back and
uncomment the <code>exclude=#kernel*</code> line in <em>/etc/yum.conf</em>, at
your own discretion.</p>

<h2>IV. Configuring Open-iSCSI</h2>

<h3>1. Configuring the NIC cards for iSCSI use</h3>

<p>In the following example, it is assumed you will be using the third
(eth2) and fourth (eth3) Network Interface Cards in the system
for iSCSI traffic.  If this is not the case, then adjust the configuration
for different NICs as necessary.</p>

<p>Add the following bold lines to each of the <em>ifcfg-ethX</em> scripts
for each iSCSI NIC.  Ensure that <code>IPADDR</code>, and <code>NETMASK</code> are correctly
set for your iSCSI subnet.  Make sure <code>ONBOOT</code> is set to <em>yes</em>.<br/>
This is set so that each interface will be automatically initialized
when the system is booted.  The <code>MTU</code> variable should be set to
<em>9000</em>, for each iSCSI NIC, this MTU setting will enable jumbo
frames.</p>

<p><em>/etc/sysconfig/network-scripts/ifcfg-eth2</em>
```bash</p>

<h1>Broadcom Corporation NetXtreme II BCM5709 Gigabit Ethernet</h1>

<p>DEVICE=eth2
HWADDR=00:11:22:33:44:aa
ONBOOT=yes
BOOTPROTO=none
NETMASK=255.255.255.0
IPADDR=10.1.2.3
TYPE=Ethernet
MTU=9000
```</p>

<p><em>/etc/sysconfig/network-scripts/ifcfg-eth3</em>
```bash</p>

<h1>Broadcom Corporation NetXtreme II BCM5709 Gigabit Ethernet</h1>

<p>DEVICE=eth3
HWADDR=00:11:22:33:44:bb
ONBOOT=yes
BOOTPROTO=none
NETMASK=255.255.255.0
IPADDR=10.1.2.4
TYPE=Ethernet
MTU=9000
```</p>

<p>Dell EqualLogic (2009) recommends enabling Flow Control, and
disabling Auto Negotiation on each iSCSI NIC.  For these settings
to be automatically applied upon each boot, add the following
code, in boldface, to the <em>/etc/rc.local</em> file.</p>

<p><em>/etc/rc.local</em>
```bash</p>

<h1>iSCSI Interface Settings for Equallogic</h1>

<p>ISCSI_IF="eth2 eth3"
ETHTOOL_OPTS="autoneg off rx on tx on"
ETHTOOL=<code>which ethtool</code></p>

<p>for i in $ISCSI_IF; do
  echo "$ETHTOOL -A $i $ETHTOOL_OPTS"
  $ETHTOOL -A $ISCSI_IF $ETHTOOL_OPTS
done
```</p>

<h4>1.1. Configuring Open-iSCSI initiator utilities</h4>

<p>You need to tune a few lines in the <em>/etc/iscsi/iscid.conf</em> file,
per vendor recommendations.  You can grep out blank and comment
lines in this file for quick inspection.  The lines we are most
concerned about are followed by comments, noting vendor-specific
recommendations.</p>

<h5>1.1.1. Notes about iscsid.conf configuration</h5>

<p>As I understand it, the configuration item, <code>node.session.timeo.replacement_timeout</code>
is for specifying the length of time, in seconds,
after which the iSCSI layer will fail back to the Device Mapper
Multipath application layer.  By default, this is <em>120</em> seconds, or 2 minutes.
According to Red Hat (2008), it is preferable to pass the path failure quickly from
the SCSI layer to your Multipath software, which would be necessary for
quick fail-over.  With the default configuration, two minutes
of I/O would be queued before failing the path. If you are familiar
with Fiber Channel path fail-over, it tends to be instantaneous.<br/>
Most people configuring iSCSI fail-over certainly don't want
a failed path to be retried for two whole minutes.  Using the default
configuration with a production database server could lead
to massive amounts of I/O being queued instead of the expected
behavior of failing over to another working path quickly.  Changing the
<code>node.session.timeo.replacement_timeout</code> value to <em>15</em> will allow the path
to failover much more quickly.</p>

<p>According to Dell (2010), if the Broadcom <strong>bnx2i</strong> interface fails
to login to an Equallogic iSCSI storage volume, the <code>node.session.initial_login_retry_max</code>
should be changed from the default value of <em>8</em> to <em>12</em>.</p>

<p>According to Dell Equallogic (2009) the options <code>node.session.cmds_max</code>,
and <code>node.session.queue_depth</code> should be changed for Equallogic
arrays.  The recommendation for <code>node.session.cmds_max</code> is
to change the value from <em>128</em> to <em>1024</em>.  The recommendation for
<code>node.session.queue_depth</code> is to change the value from <em>32</em>
to <em>128</em>.</p>

<p>Finally, according to the in-line comments in the default <em>iscsid.conf</em>,
and Dell (2010) Equallogic arrays should have <code>node.session.iscsi.FastAbort</code>
set to <em>No</em>.  If using software such as iSCSI Enterprise Target
(IET), instead of an Equallogic Array, leave the <code>node.session.iscsi.FastAbort</code>
value set to the default, which is <em>Yes</em>.</p>

<h5>1.1.2. iscsid.conf</h5>

<p><code>bash
egrep -v “^(#|$)” /etc/iscsi/iscid.conf
node.startup = automatic
node.session.timeo.replacement_timeout = 15 # default 120; RedHat recommended
node.conn[0].timeo.login_timeout = 15
node.conn[0].timeo.logout_timeout = 15
node.conn[0].timeo.noop_out_interval = 5
node.conn[0].timeo.noop_out_timeout = 5
node.session.err_timeo.abort_timeout = 15
node.session.err_timeo.lu_reset_timeout = 20
node.session.initial_login_retry_max = 12 # default 8; Dell recommended
node.session.cmds_max = 1024 # default 128; Equallogic recommended
node.session.queue_depth = 128 # default 32; Equallogic recommended
node.session.iscsi.InitialR2T = No
node.session.iscsi.ImmediateData = Yes
node.session.iscsi.FirstBurstLength = 262144
node.session.iscsi.MaxBurstLength = 16776192
node.conn[0].iscsi.MaxRecvDataSegmentLength = 262144
discovery.sendtargets.iscsi.MaxRecvDataSegmentLength = 32768
node.conn[0].iscsi.HeaderDigest = None
node.session.iscsi.FastAbort = No # default Yes; Dell / Open-iSCSI recommended**
</code></p>

<h2>IV. Targeting and logging in with iscsiadm</h2>

<h3>1. Create iSCSI interfaces</h3>

<p>The iSCSI interfaces are separate pseudo-devices used by Open-iSCSI.<br/>
From Open-iSCSI's point-of-view, these are not the same as physical
Ethernet devices.  What you will need to do is bind
each of your ethX devices to an iSCSI interface.  Technically speaking,
you could call your iSCSI interfaces eth2 and eth3, and bind them to
the corresponding physical devices in the /dev folder.  To avoid any
confusion about physical NICs and iSCSI interfaces, it is recommended
to give the interfaces different names like <code>iface0</code> and <code>iface1</code>.</p>

<p>You can create your iSCSI interfaces with the following commands.</p>

<p>```bash
iscsiadm -m iface -I iface0 -o new</p>

<p>iscsiadm -m iface -I iface1 -o new
```</p>

<p>When you pass no options to <code>iscsiadm</code> then you may examine
the iSCSI interface properties for one of the new interfaces.  Being able
to view the currently active configuration can come in handy in cases where
you need to troubleshoot.</p>

<p>```bash
iscsiadm -m iface -I iface0</p>

<h1>BEGIN RECORD 2.0-871</h1>

<p>iface.iscsi_ifacename = iface0
iface.net_ifacename = <empty>
iface.ipaddress = <empty>
iface.hwaddress = <empty>
iface.transport_name = tcp
iface.initiatorname = <empty></p>

<h1>END RECORD</h1>

<p>```</p>

<h3>2. Bind iSCSI interfaces</h3>

<p>Now that we have iSCSI interfaces, we need to bind them to a physical
Ethernet device.  You can use either a device name or MAC address.  Open-iSCSI
will not allow you to bind the interfaces with both device name
and MAC address.</p>

<h4>Binding by physical device name</h4>

<p>```bash
iscsiadm -m iface -o update -I iface0 -n iface.net_ifacename -v eth2</p>

<p>iscsiadm -m iface -o update -I iface1 -n iface.net_ifacename -v eth3
```</p>

<h4>Binding by MAC address</h4>

<p>```bash
iscsiadm -m iface -o update -I iface0 -n iface.hwaddress -v 00:aa:bb:cc:dd:ee</p>

<p>iscsiadm -m iface -o update -I iface1 -n iface.hwaddress -v 00:aa:bb:cc:dd:ff
```</p>

<p>An <strong>important note</strong> about alternate transport drivers for iSCSI
offload:</p>

<p>By default, your new iSCSI interfaces will use TCP as
the iSCSI transport. There are other offload transport drivers
available to use, such as <strong>bnx2i</strong>, <strong>iser</strong>, and <strong>cxgb3i</strong>.
According to the iscsiadm manual page and personal conversation
with a Dell storage engineer, these are experimental drivers
which are not supported or considered stable.</p>

<h4>2.1. Updating iSCSI interfaces</h4>

<p>If you bind an interface by MAC address, and have a hardware replacement
which changes the MAC address. Then the iSCSI interface bindings
will need to be updated to reflect such system changes.
You can do this with an update command:</p>

<p>```bash
iscsiadm -m iface -o update -I iface0 -n iface.hwaddress -v 00:aa:bb:cc:dd:11</p>

<p>iscsiadm -m iface -o update -I iface1 -n iface.hwaddress -v 00:aa:bb:cc:dd:33
```</p>

<h3>3. Connecting to the iSCSI array</h3>

<p>The file <em>/etc/iscsi/initiatorname.iscsi</em> should contain
an initiator name for your iSCSI client host.  You need to include
this initiator name on your iSCSI array's configuration for
this specific iSCSI client host.</p>

<p>If you have not yet started the iSCSI daemon, run the following
command before we commence with discovering targets.</p>

<h4>Start the iSCSI daemon</h4>

<p><code>bash
service iscsid start
</code></p>

<h4>3.1. Discovering targets</h4>

<p>Once the iscsid service is running
and the client's initiator name is configured on the iSCSI array,
then you may proceed with the following command to discover available
targets.  Assuming our iSCSI array had an IP address of 10.1.2.10,
the following command would return the available targets.</p>

<h5>Target discovery command</h5>

<p><code>bash
iscsiadm -m discovery -t st -p 10.1.2.10:3260
10.1.2.10:3260,1 iqn.2001-05.com.equallogic:0-8a0906-7008ec504-23d000000204bad0-hostname-vol0
10.1.2.10:3260,1 iqn.2001-05.com.equallogic:0-8a0906-7008ec504-23d000000204bad0-hostname-vol0
</code></p>

<h4>3.2. Login to target</h4>

<p>We are finally ready to login to our target.  To log in to all targets, use
the following command.  This should return successful
if everything is working correctly.</p>

<h5>Login command</h5>

<p><code>bash
iscsiadm -m node -l
</code></p>

<p>Individual targets can be logged into by specifying a whole target name.</p>

<h5>Login to a specific target by name</h5>

<p><code>bash
iscsiadm -m node -T iqn.2001-05.com.equallogic:0-8a0906-7008ec504-23d000000204bad0-hostname-vol0 -l -p 10.1.2.10:3260
</code></p>

<h4>3.3. Logoff a target</h4>

<p>Logging off is basically the same as logging into a target, except
you use the <em>-u</em> switch, instead of <em>-l</em>.</p>

<h5>Command to log out of a target</h5>

<p><code>bash
iscsiadm -m node -u
</code></p>

<h4>3.4. Session status</h4>

<p>The session command can be used to print the basic status
or more verbose output for debugging and troubleshooting.</p>

<h5>Basic session status command</h5>

<p><code>bash
iscsiadm -m session
tcp: [10] 10.1.2.10:3260,1 iqn.2001-05.com.equallogic:0-8a0906-7008ec504-23d000000204bad0-hostname-vol0
tcp: [9] 10.1.2.10:3260,1 iqn.2001-05.com.equallogic:0-8a0906-7008ec504-23d000000204bad0-hostname-vol0
</code></p>

<h5>Troubleshooting information with -P (print) flag and verbosity level 0-3</h5>

<p>```bash
iscsiadm -m session -P3
iSCSI Transport Class version 2.0-871
version 2.0-871
Target: iqn.2001-05.com.equallogic:0-8a0906-7008ec504-23d000000204bad0-hostname-vol0
Current Portal: 10.1.2.25:3260,1
Persistent Portal: 10.1.2.10:3260,1</p>

<hr />

<p>Interface:</p>

<hr />

<p>Iface Name: iface0
Iface Transport: tcp
Iface Initiatorname: iqn.1994-05.com.redhat:13c39f80866f
Iface IPaddress: 10.1.2.3
Iface HWaddress: <empty>
Iface Netdev: eth2
SID: 10
iSCSI Connection State: LOGGED IN
iSCSI Session State: LOGGED_IN
Internal iscsid Session State: NO CHANGE</p>

<hr />

<p>Negotiated iSCSI params:</p>

<hr />

<p>HeaderDigest: None
DataDigest: None
MaxRecvDataSegmentLength: 262144
MaxXmitDataSegmentLength: 65536
FirstBurstLength: 65536
MaxBurstLength: 262144
ImmediateData: Yes
InitialR2T: No
MaxOutstandingR2T: 1</p>

<hr />

<p>Attached SCSI devices:</p>

<hr />

<p>Host Number: 27 State: running
scsi27 Channel 00 Id 0 Lun: 0
Attached scsi disk sdc State: running
```</p>

<h2>V. Configuring Device Mapper Multipath</h2>

<h3>1. Notes about the Device Mapper Multipath example</h3>

<p>The final major step is to configure Device Mapper Multipath
using the configuration file <em>/etc/multipath.conf</em>.  By default there will be a <code>devnode “*”</code>
line in the <code>blacklist</code> section, thereby disabling Multipath
for all devices in the system.  If you want to assign persistently
bound names to iSCSI devices, be sure to set <code>user_friendly_names</code>
to <em>yes</em>, as seen in the example on the following page.  If you do not
use friendly names with Device Mapper, you'll end up with device
names such as <em>/dev/mapper/__36090afffffffffffffffffffffffffff</em>.
When using friendly names, you will need to specify an <code>alias</code>
line in the <code>multipaths</code> section after Open-iSCSI is configured
and working correctly with your iSCSI target or array.</p>

<p>You will want to blacklist any local devices that do not have multiple
paths to be managed in the <em>blacklist</em> section. It is recommended
to blacklist the local SCSI disks by World Wide Identifier (WWID).
The WWID is a unique identifier for any block device and may be retrieved with the command
<code>scsi_id -g -u -s /block/sdX</code>, where <em>X</em> is the letter identifier of the local SCSI disk.
It cannot hurt to also blacklist local devices by physical device name with
a <code>devnode</code> line and a Perl-compatible regular expression string.<br/>
The reason for blacklisting a second time with a regular expression
is in case the <code>scsi_id</code> program fails to read the WWID from sector
zero of a local device.  See the blacklist section in the configuration
example.  The WWID line and <code>devnode “^sd[a]$”</code> line both serve as a
blacklist for device sda.</p>

<p>Once you have discovered targets and logged in to the iSCSI array, you can use the <code>iscsiadm -m
session -P3</code> command to find the physical device names of your
iSCSI volumes.  Then you can use
the <code>scsi_id -g -u -s /block/sdX</code> command to find the WWID of
your iSCSI volumes. Once you have the WWIDs of your iSCSI volumes,
you can configure friendly name aliases in the <code>multipaths</code>
section, as seen in the example <em>multipath.conf</em> below.</p>

<p>The device section in the configuration example is pertinent
to an Equallogic PS Series array.  The lines followed by comments are
of particular importance, and are the recommended defaults
for Equallogic devices. You can override the parameters on a
per-volume basis in the <code>multipaths</code> section.</p>

<p>For best performance, the value for <code>rr_min_io</code> should
be in the range of <em>10-20</em> for database environments.
For more sequential loads, which may be seen on file servers,
performance might be better if
<em>rr_min_io</em> is set in the range
<em>100-512</em>.  Any <code>rr_min_io</code> value over <em>200</em>
will require <code>cmds_max</code> and <code>queue_depth</code> parameters in
<em>iscsid.conf</em> to be increased.</p>

<h4>1.1. multipath.conf</h4>

<p>```bash
defaults {
  user_friendly_names yes
}</p>

<p>blacklist {
  wwid 360924fffffffffffffffffffffffffff
  devnode "<sup>sd[a]$"</sup>
  devnode "<sup>(ram|raw|loop|fd|md|dm-|sr|scd|st)[0-9]*"</sup>
  devnode "<sup>hd[a-z][[0-9]*]"</sup>
  devnode "<sup>cciss!c[0-9]d[0-9]<em>[p[0-9]</em>]"</sup>
}</p>

<p>devices {
  device {</p>

<pre><code>vendor "EQLOGIC" # Manufacturer
product "100E-00" # Model identifier
path_grouping_policy multibus 
getuid_callout "/sbin/scsi_id -g -u -s /block/%n" {text:change} 
no_path_retry queue # If no paths, queue I/O 
path_checker readsector0 
failback immediate 
path_selector "round-robin 0" 
rr_min_io 10 # 10-20 Database / 100-512 File server
rr_weight priorities 
</code></pre>

<p>  }
}</p>

<p>multipaths {
  multipath {</p>

<pre><code>wwid 36090afffffffffffffffffffffffffff 
alias u02 
</code></pre>

<p>  }
}
```</p>

<h4>1.2. Start and test Device Mapper Multipath</h4>

<p>Device Mapper Multipath can be started with the following command
once it has been configured.</p>

<h5>Start multipathd service</h5>

<p><code>bash
service multipathd start
</code></p>

<p>You can list the Multipath topology with the following command
to verify everything is working correctly.</p>

<h5>Multipath listing</h5>

<p><code>bash
multipath -ll -v2
u02 (36090a04850ec0870d0ba04020000d023) dm-0 EQLOGIC,100E-00
[size=100G][features=0][hwhandler=0][rw]
\_ round-robin 0 [prio=2][active]
\_ 10:0:0:0 sdb 8:16 [active][ready]
\_ 11:0:0:0 sdc 8:32 [active][ready]
</code></p>

<h4>1.3. Reload udev to test friendly alias names</h4>

<p>Once Multipath has been verified to be working correctly,
you may need to run the following <code>udev</code> command to create
devices with friendly names.  This only applies if you
have chosen to use persistent Multipath binding by defining
<code>alias</code> lines in the <code>multipaths</code> section of <em>multipath.conf</em>.<br/>
Reloading udev will automagically create your aliased devices in the <em>/dev/mapper</em>
folder.</p>

<h5>Reload udev command</h5>

<p><code>bash
udevcontrol reload_rules
</code></p>

<p>You may proceed to <code>fdisk</code> and
format the aliased device, just as you would any other SCSI disk
device appearing in the <em>/dev</em> directory.</p>

<h2>VI. Final steps</h2>

<h3>1. Set services to start automatically.</h3>

<p>Once you have everything up and running, with regards to Open-iSCSI
and Device Mapper Multipath, run the following
commands to ensure services get started automatically during
server boot up.</p>

<h4>Set iscsid to start at boot</h4>

<p><code>bash
chkconfig iscsid on
</code></p>

<h4>Set iscsi to login to target at boot</h4>

<p><code>bash
chkconfig iscsi on
</code></p>

<h4>Set multipathd to start at boot</h4>

<p><code>bash
chkconfig multipathd on
</code></p>

<p>Edit <em>/etc/fstab</em>, and add a line with the <code>_netdev</code> keyword
for all your volumes to be mounted. Using the <code>_netdev</code> keyword
ensures this device will not be mounted before the networking
subsystem has started.</p>

<p>We have observed Open-iSCSI failing both paths temporarily while
updating firmware on the group node.  This could
result in a machine remounting a filesystem in read-only mode
upon error unless an <code>errors=continue</code> option is added to
<em>/etc/fstab</em>.</p>

<p><em>/etc/fstab</em>
<code>bash
LABEL=/             /       ext3     defaults 1 1
LABEL=/u01          /u01    ext3     defaults 1 2
/dev/mapper/u02p1   /u02    ext3     _netdev,defaults,errors=continue 0 0
tmpfs               /dev/shm  tmpfs  defaults 0 0
devpts              /dev/pts  devpts gid=5,mode=620 0 0
sysfs               /sys    sysfs    defaults 0 0
proc                /proc   proc     defaults 0 0
LABEL=SWAP-sda5     swap    swap     defaults 0 0
</code></p>

<h3>2. Final testing</h3>

<h4>2.1. Reboot</h4>

<p>Reboot your server, and make sure everything comes up. The iscsi
initialization script should log the server in to the iSCSI targets.
Multipath should be started, and it should see both of its paths to the iSCSI array. If
everything is working correctly, the <em>fstab</em> entry should automatically
mount any iSCSI volumes.</p>

<h4>2.2. Test your Multipath software</h4>

<p>The easiest way to test your Multipath software is to take down
one of the Ethernet devices manually.  To do this, run the following command.</p>

<h5>Faking network failure</h5>

<p><code>bash
ifdown eth3
</code></p>

<p>After about 15 seconds you should see something like this in the
messages log.</p>

<h5>Path failure log message</h5>

<p><code>bash
kernel: device-mapper: multipath: Failing path X:XX
multipathd: &lt;alias&gt;: remaining active paths: 1
</code></p>

<p>Then, bring the device back up manually.</p>

<h5>End fake network failure</h5>

<p><code>bash
ifup eth3
</code></p>

<p>Again, after about 15 seconds you should see something like this
in the messages log.</p>

<h5>Path resolved log message</h5>

<p><code>bash
multipathd: X:XX: reinstated
multipathd: &lt;alias&gt;: remaining active paths: 2
</code></p>

<p>Repeat this test for every other NIC being used for iSCSI and make
sure every iSCSI Network Interface fails-over from faulty paths,
and reinstate all active iSCSI paths gracefully.</p>

<h2>References and recommended reading</h2>

<p>Aizman, Alex, and Dmitry Yusupov. 2007. Open-iSCSI – RFC3720
architecture and implementation. <em>Open-iSCSI project</em>. <a href="http://www.open-iscsi.org/index.html#docs">http://www.open-iscsi.org/index.html#docs</a>.</p>

<p>Anon. Device-mapper Resource Page. <em>Device-Mapper-Multipath
Project</em>. <a href="http://sources.redhat.com/dm/">http://sources.redhat.com/dm/</a>.</p>

<p>Dell EqualLogic, Inc. 2008. <em>PS Series Array Network Performance
Guidelines</em>. 3rd ed. Nashua, NH: Dell, Inc, June. <a href="http://www.equallogic.com/uploadedfiles/Resources/Tech_Reports/tr-network-guidelines-TR1017.pdf">http://www.equallogic.com/uploadedfiles/Resources/Tech_Reports/tr-network-guidelines-TR1017.pdf</a>.</p>

<p>———. 2009. <em>Red Hat Linux v5.x Software iSCSI Initiator Configuration,
MPIO and tuning Guide</em>. Nashua, NH: Dell, Inc, December. <a href="http://www.equallogic.com/resourcecenter/assetview.aspx?id=8727">http://www.equallogic.com/resourcecenter/assetview.aspx?id=8727</a>.</p>

<p>Red Hat, Inc. 2007. How do I configure the iscsi-initiator in
Red Hat Enterprise Linux 5? <em>Red Hat Knowledgebase</em>. June 25.
<a href="http://access.redhat.com/kb/docs/DOC-6388">https://access.redhat.com/kb/docs/DOC-6388</a>.</p>

<p>———. 2008. How can I improve the failover time of a faulty path
when using device-mapper-multipath over iSCSI? <em>Red Hat Knowledgebase</em>.
October 1. <a href="https://access.redhat.com/kb/docs/DOC-2877">https://access.redhat.com/kb/docs/DOC-2877</a>.</p>

<p>———. 2010a. <em>DM Multipath</em>. <a href="http://www.redhat.com/docs/en-US/Red_Hat_Enterprise_Linux/5.5/html/DM_Multipath/index.html">http://www.redhat.com/docs/en-US/Red_Hat_Enterprise_Linux/5.5/html/DM_Multipath/index.html</a>.</p>

<p>———. 2010b. Kickstart Installations. In <em>Installing Red Hat
Enterprise Linux 5 for all architectures</em>, Chap. 31. Research
Triangle Park, NC: Red Hat, Inc. <a href="http://www.redhat.com/docs/en-US/Red_Hat_Enterprise_Linux/5.5/html/Installation_Guide/index.html">http://www.redhat.com/docs/en-US/Red_Hat_Enterprise_Linux/5.5/html/Installation_Guide/index.html</a>.</p>
]]></content>
  </entry>
  
</feed>
